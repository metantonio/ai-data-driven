# LLM Configuration
# Options: mock, ollama, vllm (openai-compatible)
LLM_PROVIDER=ollama

# Base URL for the LLM API
# For Ollama: http://localhost:11434/api/generate
# For vLLM/OpenAI: http://localhost:8000/v1/chat/completions
LLM_API_URL=http://localhost:11434/api/generate

# Model Name
# e.g., llama2, mistral, codellama for Ollama
# e.g., facebook/opt-125m for vLLM
LLM_MODEL=mistral
